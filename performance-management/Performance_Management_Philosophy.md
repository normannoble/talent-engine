# Performance Management Philosophy & Guidelines

## Purpose

This document establishes the foundational principles for performance management within our outcome-based leveling framework. It ensures that performance conversations, evaluations, and documentation remain aligned with how we define success at each level.

Performance management exists to answer three questions:
1. **How is this person doing?** (Current performance)
2. **Where are they heading?** (Development trajectory)
3. **What support do they need?** (Enablement)

It is not primarily about determining compensation or promotions—though it informs both. It's about creating clarity, enabling growth, and ensuring alignment between individual contribution and organizational success.

---

## Core Principles

### 1. Outcome-Based, Not Activity-Based

Just as our job descriptions define success through outcomes rather than task lists, performance evaluation should focus on **what was achieved and the impact created**, not merely what activities were performed.

**Instead of:** "Attended all sprint ceremonies and completed assigned tickets"  
**Evaluate:** "Delivered features that met quality standards with reliable estimation and minimal rework"

**Instead of:** "Conducted weekly 1:1s with all direct reports"  
**Evaluate:** "Team members who understand expectations, receive regular feedback, and are developing their skills"

The job description's Success Outcomes section is the primary reference for evaluation. If those outcomes are true, the person is successful.

### 2. Level-Appropriate Expectations

Performance is always evaluated relative to the person's current level and role. The same output may represent:
- Exceptional performance for an IC2
- Meets Expectation for an IC3
- Below Expectation for an IC4

**The question is never "Did they do good work?"**  
**The question is "Did they deliver what success looks like at their level?"**

This requires managers to deeply understand the distinctions between levels and resist the temptation to evaluate everyone against the same standard.

### 3. Sustained Performance, Not Isolated Events

Performance evaluation should reflect patterns over time, not individual moments—positive or negative.

- A single excellent project doesn't make someone ready for promotion
- A single mistake doesn't define someone's performance
- Consistency matters more than peaks

Evaluation periods exist to capture these patterns. Recency bias is a constant threat—document throughout the period, not just at evaluation time.

### 4. Growth and Plateau Are Both Valid

Our framework explicitly acknowledges that plateauing at IC3, M1, or M2 represents strong career success. Performance management should:

- Celebrate sustained excellence at current level
- Not treat "not pursuing promotion" as a problem to solve
- Recognize that compensation can grow within levels
- Support development goals that aren't about leveling up

Some people want to grow toward the next level. Others want to deepen mastery at their current level. Both are valid, and performance conversations should honor what the individual actually wants.

### 5. Feedback Is Continuous, Evaluation Is Periodic

**Feedback** should happen in real-time or near-real-time:
- After a project ships
- When a behavior needs adjustment
- When something goes well
- In regular 1:1s

**Evaluation** synthesizes feedback into a broader assessment:
- Quarterly check-ins provide course correction
- Semi-annual reviews capture the full picture
- Promotion discussions require sustained evidence

No evaluation should contain surprises. If feedback has been continuous, the evaluation simply summarizes what's already been discussed.

### 6. Shared Accountability

Performance management is a partnership:

**The Employee:**
- Owns their development and career
- Self-assesses honestly
- Seeks feedback proactively
- Documents their own impact
- Raises concerns early

**The Manager:**
- Provides clear expectations
- Gives timely, actionable feedback
- Creates opportunities for growth
- Evaluates fairly and consistently
- Advocates for their people

**The Organization:**
- Provides clear frameworks (levels, JDs, expectations)
- Ensures calibration and consistency
- Supports manager development
- Creates growth opportunities
- Rewards performance appropriately

---

## The Performance Management Cycle

### Continuous Activities (Ongoing)

| Activity | Frequency | Purpose |
|----------|-----------|---------|
| 1:1 Meetings | Weekly or bi-weekly | Relationship, feedback, blockers, development |
| Real-time feedback | As events occur | Course correction, recognition |
| Work documentation | Ongoing | Evidence gathering for evaluations |
| Goal progress tracking | Ongoing | Alignment and accountability |

### Periodic Activities (Scheduled)

| Activity | Frequency | Purpose |
|----------|-----------|---------|
| Quarterly check-in | Every 3 months | Course correction, goal adjustment, early signals |
| Peer assessment | Semi-annually | Colleague perspectives on strengths and development areas |
| Semi-annual review | Twice yearly | Comprehensive evaluation, compensation input |
| Readiness assessment | As triggered | Level transition assessment |
| Calibration | Each review cycle | Cross-team consistency |

---

## Connecting Performance to the Framework

### Using Job Descriptions as Evaluation Criteria

Each job description contains a **Success Outcomes** section that defines what success looks like. This is the primary evaluation rubric.

**For each outcome area, assess:**
1. Is this outcome being achieved?
2. To what degree?
3. What's the evidence?

**Example (IC3 Senior Software Engineer):**

| Success Outcome | Assessment | Evidence |
|-----------------|------------|----------|
| Software that is well-architected and serves as a model for quality | Meets Expectation | Code reviews consistently positive; architecture decisions in Project X adopted by team |
| Junior engineers who are growing faster because of effective mentorship | Exceeds Expectation | Mentored 2 IC1s who both received strong reviews; created onboarding documentation |
| Technical debt identified and addressed proactively | Below Expectation | Identified issues but needed prompting to prioritize; could be more proactive |

### Using Progression Guides for Development

The Progression Guides describe **what capabilities to develop** to reach the next level. They're useful for:

- Identifying growth areas in development conversations
- Setting development goals that align with career trajectory
- Understanding the gap between current level and next level

**Example conversation:**
> "You're performing strongly at IC2. Looking at the IC2→IC3 Progression Guide, the areas that would stretch you toward Senior are: taking on more ambiguous projects, mentoring others, and driving technical improvements. Which of these interests you most?"

### Using Readiness Assessments for Promotion

The Readiness Assessment templates provide structured evaluation of promotion readiness. They should be used:

- When someone has achieved **repeated Exceeds Expectation** ratings
- As a diagnostic to identify specific gaps before promotion discussion
- To ensure consistent promotion standards across teams

**Triggering a Readiness Assessment:** When an employee receives "Exceeds Expectation" in two consecutive review cycles, this should trigger a formal Readiness Assessment for the next level. This ensures we recognize sustained above-level performance and creates a structured path toward promotion.

**Readiness Assessments are not performance reviews.** They're a specific tool for a specific purpose: evaluating whether someone is ready for the next level.

---

## Rating Scale

### Performance Rating Definitions

Performance is evaluated on a five-level scale focused on performance at the current role and level:

| Rating | Definition | Indicators |
|--------|------------|------------|
| **Unsatisfactory** | Performance significantly below expectations for the role and level. Core outcomes are not being achieved. | Fundamental gaps in role execution; requires immediate intervention; may indicate role mismatch or serious performance issues. |
| **Below Expectation** | Performance falling short of expectations in meaningful ways. Some outcomes achieved, but important gaps exist. | Inconsistent delivery; gaps in key outcome areas; requires focused development and support to reach expectations. |
| **Meets Expectation** | Consistently achieving success outcomes defined for the role at this level. Solid, reliable performance. | Reliable delivery on commitments; achieving what's expected at level; no significant gaps; may or may not be pursuing promotion. |
| **Exceeds Expectation** | Performing above level expectations. Consistently demonstrating capabilities associated with the next level. | Delivering outcomes typical of the next level; ready to take on expanded scope; strong candidate for promotion if sustained. |
| **Exceptional** | Rare, outstanding performance well beyond level expectations. Making extraordinary impact. | Transformative contributions; significantly exceeding next-level expectations; performance that stands out across the organization. |

### Rating Guidance

**"Meets Expectation" is a positive rating.** It indicates someone is doing exactly what success looks like at their level. This is the most common rating for solid performers and should not be treated as criticism or a negative signal.

**"Exceeds Expectation" signals promotion readiness.** If someone consistently receives this rating, they're demonstrating next-level capability. Two consecutive "Exceeds Expectation" ratings should trigger a formal Readiness Assessment.

**"Exceptional" is rare by design.** This rating should be reserved for truly extraordinary performance—not just "very good." Most strong performers will be rated "Meets" or "Exceeds," and that's appropriate.

**Avoid rating inflation.** When everyone is rated "Exceeds" or "Exceptional," the scale loses meaning and creates challenges for recognizing genuine high performers.

### Expected Distribution

While not a forced distribution, a healthy organization should typically see:

| Rating | Expected Range | Notes |
|--------|---------------|-------|
| Unsatisfactory | <5% | Rare; should trigger immediate action |
| Below Expectation | 5-15% | Developmental focus required |
| Meets Expectation | 50-70% | The healthy center—solid performers |
| Exceeds Expectation | 15-25% | Strong performers, promotion candidates |
| Exceptional | <5% | Truly rare, outstanding performance |

If your team's distribution looks significantly different, examine whether ratings are being applied consistently against level expectations.

---

## Calibration

Calibration ensures consistent standards across managers and teams. Without calibration:
- Some managers rate generously, others harshly
- Similar performance gets different ratings
- Promotion standards become inconsistent
- Trust in the system erodes

**Calibration involves:**
- Managers presenting their ratings with evidence
- Discussion of borderline cases
- Adjustment to ensure cross-team consistency
- Shared understanding of what each rating means at each level

Calibration happens each review cycle, before ratings are finalized and shared with employees.

---

## The Three-Input Assessment Model

Performance evaluation draws on three distinct perspectives: self-assessment, peer assessment, and manager assessment. Each provides unique value, and the manager is responsible for synthesizing them into a coherent, calibrated final assessment.

### How Each Input Functions

| Source | Role | What It Provides |
|--------|------|------------------|
| **Self-Assessment** | Context & Evidence | Employee's perspective on accomplishments, challenges faced, and context only they have visibility into |
| **Peer Assessment** | Validation & Blind Spots | Collaboration quality, technical contributions, reliability, day-to-day behaviors—things the manager may not directly observe |
| **Manager Assessment** | Synthesis & Decision | Final evaluation that weighs all inputs, applies level-appropriate expectations, and aligns with calibration |

### Manager as Synthesizer

The manager owns the final assessment and is accountable for synthesizing all inputs thoughtfully. This means:

- **Genuinely considering** self and peer input (not dismissing or ignoring it)
- **Explaining divergence** when their assessment differs significantly from self or peer perspectives
- **Using peer feedback** to validate or challenge their own observations
- **Applying judgment** about which inputs are most relevant in each situation
- **Being accountable** for the final rating through calibration

There are no explicit numerical weights (e.g., "50% manager, 25% self, 25% peer"). Such weights create false precision and remove the judgment that managers are accountable for. Different situations warrant different emphasis—a remote employee's peers may have more direct visibility than their manager, while a manager may have context about strategic priorities that peers lack.

**The principle:** All three perspectives matter. None is determinative on its own. The manager synthesizes them into a coherent assessment and must be prepared to explain their reasoning.

### Peer Assessment Process

**Peer Selection:**
1. Employee nominates 3-5 peers who have meaningful collaboration experience
2. Manager reviews and approves nominations (may suggest additions or substitutions)
3. Final list should include people with genuine working relationship, not just friendly colleagues

**What Peers Assess:**
Peers provide feedback on two questions, referencing the employee's job description:
1. Where do you see this person performing well against their role expectations?
2. Where do you see opportunities for this person to develop or improve?

Peers should focus on areas where they have direct observation—collaboration, communication, technical contribution, reliability, and how the person shows up day-to-day.

**How Peer Feedback Is Used:**
- Manager receives peer feedback directly
- Manager summarizes themes and patterns (peer feedback is not shared verbatim with the employee)
- Peer identities are not disclosed in the summary
- Manager incorporates peer perspective into their assessment and review conversation

**Selecting Good Peers:**
- Choose people you've actually worked with during the review period
- Include peers from different contexts (project work, cross-functional collaboration, etc.)
- Don't select only people who will say nice things—honest feedback is more valuable
- Manager may adjust nominations to ensure balanced perspective

### When Perspectives Diverge

It's common for self, peer, and manager perspectives to differ. This is valuable information, not a problem to solve.

**Self rates higher than manager:** May indicate blind spots, unclear expectations, or context the manager is missing. Explore in the review conversation.

**Peers rate higher than manager:** May indicate the manager lacks visibility into day-to-day contributions, or that the employee is strong collaborator but struggling in areas only the manager sees.

**Peers rate lower than manager or self:** May indicate collaboration or interpersonal issues not visible to the manager, or that the employee prioritizes manager-visible work over peer-facing work.

**Significant divergence warrants conversation.** The goal is understanding, not necessarily agreement.

---

## Common Pitfalls

### Evaluation Pitfalls

**Recency Bias:** Overweighting recent events at the expense of the full evaluation period.
*Solution:* Document throughout the period; review notes before writing evaluations.

**Halo/Horn Effect:** Letting one positive or negative trait color the entire evaluation.
*Solution:* Evaluate each outcome area independently with specific evidence.

**Central Tendency:** Rating everyone as "Meets Expectation" to avoid difficult conversations.
*Solution:* Require specific evidence for each rating; calibrate across teams.

**Leniency/Strictness:** Consistently rating higher or lower than peers.
*Solution:* Calibration; manager training; evidence-based discussion.

**Similar-to-Me Bias:** Rating people who are similar to you more favorably.
*Solution:* Focus on outcomes, not style; diverse calibration panels.

**Evaluating Against Wrong Level:** Comparing IC2 performance to IC3 expectations.
*Solution:* Always reference the job description for the person's actual level.

### Feedback Pitfalls

**Saving Feedback:** Waiting for formal reviews to share concerns.
*Solution:* Feedback should be real-time; reviews should contain no surprises.

**Vague Feedback:** "You need to communicate better" without specifics.
*Solution:* SBI model (Situation-Behavior-Impact) with concrete examples.

**Feedback Without Support:** Identifying problems without helping solve them.
*Solution:* Partner on development; create opportunities to practice.

### Goal-Setting Pitfalls

**Activity Goals:** "Attend training" instead of outcome goals.
*Solution:* Frame goals in terms of capability or impact, not activities.

**Too Many Goals:** Diluting focus across too many priorities.
*Solution:* 3-5 meaningful goals per period maximum.

**Set and Forget:** Setting goals then never revisiting them.
*Solution:* Review goal progress in every 1:1 and quarterly check-in.

---

## Performance Documentation Types

### What Each Document Does

| Document | Purpose | Frequency | Owner |
|----------|---------|-----------|-------|
| **Self-Assessment** | Employee reflects on their own performance against expectations | Semi-annually | Employee |
| **Peer Assessment** | Colleagues provide perspective on strengths and development areas | Semi-annually | Peers (selected collaboratively) |
| **Manager Assessment** | Manager evaluates performance, synthesizing all inputs | Semi-annually | Manager |
| **Quarterly Check-in** | Lightweight alignment on progress, goals, and development | Quarterly | Employee + Manager |
| **Semi-Annual Review** | Comprehensive evaluation of the period | Twice yearly | Manager (synthesizing all inputs) |
| **Performance Improvement Plan (PIP)** | Structured plan when performance doesn't meet expectations | As needed | Manager + HR |
| **Readiness Assessment** | Evaluation of promotion readiness (triggered by repeated Exceeds) | As triggered | Employee + Manager |

### What Each Document Should NOT Do

- **Self-Assessments** should not be exercises in false modesty or self-promotion
- **Peer Assessments** should not be popularity contests or vehicles for grievances
- **Manager Assessments** should not contain surprises or be punitive
- **Quarterly Check-ins** should not be perfunctory box-checking
- **Semi-Annual Reviews** should not be the only time feedback is given
- **PIPs** should not be a pretext for termination—they're genuine improvement opportunities
- **Readiness Assessments** should not be used as performance reviews

---

## Guiding Questions

### For Self-Assessments

**Performance:**
- Which success outcomes from my job description am I achieving well?
- Where am I falling short of expectations?
- What am I most proud of this period?
- What would I do differently if I could?

**Growth:**
- What new capabilities have I developed?
- What have I learned about myself?
- What do I want to focus on next?
- Am I on the path I want to be on?

**Support:**
- What's getting in my way?
- What support do I need from my manager?
- What opportunities would help me grow?
- What feedback would be helpful?

### For Peer Assessments

**Strengths (reference the person's job description):**
- Where do you see this person performing well against their role expectations?
- What contributions have they made that stood out to you?
- How do they show up as a collaborator and teammate?
- What do they do that makes others around them more effective?

**Development (reference the person's job description):**
- Where do you see opportunities for this person to grow or improve?
- What would make them even more effective in their role?
- Are there areas where their impact could be stronger?
- What feedback might be helpful for their development?

**Guidance for peers:**
- Be specific—provide examples where possible
- Focus on work and behaviors, not personality
- Be honest and constructive—vague praise isn't helpful
- Only comment on areas where you have direct observation

### For Manager Assessments

**Performance:**
- For each success outcome in the JD: Is it being achieved? What's the evidence?
- What has this person contributed that stands out?
- Where are there gaps relative to level expectations?
- How does their performance compare to peers at the same level?

**Trajectory:**
- Is this person developing and growing?
- Are they on track for their stated goals?
- Are they demonstrating capabilities at the next level?
- What's holding them back?

**Support:**
- What can I do to help them succeed?
- What opportunities should I create?
- What feedback have I not yet given?
- What resources or support do they need?

---

## Principles for Difficult Conversations

When performance isn't meeting expectations, conversations should be:

**Direct:** Don't soften the message so much that it gets lost.  
**Specific:** Use concrete examples, not generalizations.  
**Timely:** Don't wait—address issues when they're current.  
**Respectful:** Critique the work, not the person.  
**Supportive:** Focus on how to improve, not just what's wrong.  
**Documented:** Ensure mutual understanding of what was discussed.

**Framework for delivering difficult feedback:**

1. **State the observation:** What specifically happened?
2. **Explain the impact:** Why does it matter?
3. **Explore together:** What's going on? What's getting in the way?
4. **Agree on next steps:** What will change? How will you support?
5. **Follow up:** Check in on progress; don't let it drop.

---

## Relationship to Compensation and Promotion

### Performance → Compensation

Performance ratings inform compensation decisions but don't determine them mechanically. Compensation considers:

- Performance relative to level expectations
- Market rates for the role and level
- Internal equity with peers
- Budget and organizational constraints
- Tenure and trajectory within level

**"Meets Expectation" at level should result in competitive, fair compensation.** Higher ratings should be recognized with meaningful differentiation.

### Performance → Promotion

Promotion requires:

1. **Sustained performance at current level:** Solidly successful at your current level
2. **Demonstrated capability at next level:** Already showing success outcomes of the next level
3. **Organizational need:** A position at that level exists or is needed
4. **Readiness assessment:** Structured evaluation confirms readiness

**The Promotion Path:**
- Consistent "Exceeds Expectation" ratings signal someone is performing above level
- Two consecutive "Exceeds Expectation" ratings trigger a formal Readiness Assessment
- The Readiness Assessment evaluates whether the person is ready for the next level
- Promotion decision follows successful Readiness Assessment and organizational need

**Performance reviews inform promotion decisions but are not promotion decisions.** A strong performance review is necessary but not sufficient for promotion.

---

## Summary

Effective performance management in our framework:

- **Evaluates outcomes**, not activities—using job descriptions as the rubric
- **Calibrates to level**—same output means different things at different levels
- **Captures patterns**, not moments—sustained performance over time
- **Synthesizes three perspectives**—self, peer, and manager each contribute unique value
- **Honors all paths**—growth and plateau are both valid
- **Connects continuous feedback to periodic evaluation**—no surprises
- **Shares accountability**—employee, manager, and organization each play a role
- **Triggers development pathways**—repeated strong performance leads to readiness assessment

When done well, performance management creates clarity, enables growth, and builds trust. When done poorly, it becomes bureaucratic theater that no one values.

Our goal is the former.
